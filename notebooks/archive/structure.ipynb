{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91fbfa61-8904-4c1e-899f-2840e0f234d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧼 BATSE\n",
      "0 duplicate triggers\n",
      "gal_lon     843\n",
      "gal_lat     843\n",
      "time_sec    843\n",
      "tjd         843\n",
      "datetime    843\n",
      "dtype: int64\n",
      "\n",
      "🧼 Supernovae\n",
      "0 duplicate names\n",
      "maxabsmag       1508\n",
      "redshift        1507\n",
      "distance_mpc    1507\n",
      "type             131\n",
      "dec                0\n",
      "dtype: int64\n",
      "\n",
      "🧼 OSC\n",
      "0 duplicate names\n",
      "redshift    4\n",
      "name        0\n",
      "ra_deg      0\n",
      "dec_deg     0\n",
      "datetime    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and nulls\n",
    "print(\"\\n🧼 BATSE\")\n",
    "print(batse.duplicated(subset=[\"trigger\"]).sum(), \"duplicate triggers\")\n",
    "print(batse.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\n🧼 Supernovae\")\n",
    "print(sne.duplicated(subset=[\"name\"]).sum(), \"duplicate names\")\n",
    "print(sne.isnull().sum().sort_values(ascending=False).head())\n",
    "\n",
    "print(\"\\n🧼 OSC\")\n",
    "print(osc.duplicated(subset=[\"name\"]).sum(), \"duplicate names\")\n",
    "print(osc.isnull().sum().sort_values(ascending=False).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "669d9845-dfae-4a79-ab71-b8663a23a9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 BATSE Schema:\n",
      "trigger                  float64\n",
      "fluence_ch1              float64\n",
      "err_ch1                  float64\n",
      "fluence_ch2              float64\n",
      "err_ch2                  float64\n",
      "fluence_ch3              float64\n",
      "err_ch3                  float64\n",
      "fluence_ch4              float64\n",
      "err_ch4                  float64\n",
      "peak_flux_64ms           float64\n",
      "err_flux_64ms            float64\n",
      "time_flux_64ms           float64\n",
      "peak_flux_256ms          float64\n",
      "err_flux_256ms           float64\n",
      "time_flux_256ms          float64\n",
      "peak_flux_1024ms         float64\n",
      "err_flux_1024ms          float64\n",
      "time_flux_1024ms         float64\n",
      "name                      object\n",
      "burst_id                  object\n",
      "tjd                      float64\n",
      "time_sec                 float64\n",
      "ra_deg                   float64\n",
      "dec_deg                  float64\n",
      "gal_lon                  float64\n",
      "gal_lat                  float64\n",
      "error_radius             float64\n",
      "geocenter_angle          float64\n",
      "overwrite_flag            object\n",
      "overwritten_flag          object\n",
      "jd                       float64\n",
      "datetime                  object\n",
      "fluence_total_erg_cm2    float64\n",
      "fluence_total_J_m2       float64\n",
      "dtype: object\n",
      "\n",
      "📘 SNe Schema:\n",
      "name             object\n",
      "ra              float64\n",
      "dec             float64\n",
      "redshift        float64\n",
      "datetime         object\n",
      "maxabsmag       float64\n",
      "distance_mpc    float64\n",
      "type             object\n",
      "dtype: object\n",
      "\n",
      "📘 OSC Schema:\n",
      "name            object\n",
      "ra_deg         float64\n",
      "dec_deg        float64\n",
      "datetime        object\n",
      "claimedtype     object\n",
      "redshift       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"📘 BATSE Schema:\")\n",
    "print(batse.dtypes)\n",
    "\n",
    "print(\"\\n📘 SNe Schema:\")\n",
    "print(sne.dtypes)\n",
    "\n",
    "print(\"\\n📘 OSC Schema:\")\n",
    "print(osc.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf669dbe-fced-4792-90c3-2b61ad01aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Markdown schema log saved to DATASET_SCHEMA_LOG.md\n"
     ]
    }
   ],
   "source": [
    "md_lines = []\n",
    "\n",
    "datasets = {\n",
    "    \"batse_master_grb_registry.csv\": batse,\n",
    "    \"sne_1990s_cleaned.csv\": sne,\n",
    "    \"open_supernova_catalog.csv\": osc\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    md_lines.append(f\"###  {name}\")\n",
    "    md_lines.append(\"\")\n",
    "    md_lines.append(f\"- **Rows**: {len(df)}\")\n",
    "    md_lines.append(f\"- **Columns**: {len(df.columns)}\")\n",
    "    md_lines.append(\"- **Schema**:\")\n",
    "    for col in df.columns:\n",
    "        md_lines.append(f\"  - `{col}`: {df[col].dtype}\")\n",
    "    md_lines.append(\"\")\n",
    "\n",
    "# Save as .md or print\n",
    "markdown_doc = \"\\n\".join(md_lines)\n",
    "with open(registry_dir / \"DATASET_SCHEMA_LOG.md\", \"w\") as f:\n",
    "    f.write(markdown_doc)\n",
    "\n",
    "print(\" Markdown schema log saved to DATASET_SCHEMA_LOG.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d583b58-1a1d-409b-9e9f-110f50e12cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scanning CSV files for RA/Dec variants...\n",
      "\n",
      "⚠️ Skipped bruno_batse_icecube_crossmatch.csv due to error: No columns to parse from file\n",
      "⚠️ Skipped bruno_grb_icecube_matches.csv due to error: No columns to parse from file\n",
      "⚠️ Skipped bruno_sn_icecube_matches.csv due to error: No columns to parse from file\n",
      "⚠️ Skipped grb_icecube_crossmatch.csv due to error: No columns to parse from file\n",
      "📁 data\\exports\\sn_bruno_candidates.csv\n",
      "    Columns: ['name', 'ra_deg', 'dec_deg', 'discoverdate', 'claimedtype', 'redshift', 'distance_m', 'fluence']\n",
      "\n",
      "📁 data\\raw\\fermi_swift_grb_catalog.csv\n",
      "    Columns: ['id', 'GRB_name', 'ra_deg', 'dec_deg', 'Error_Radius', 'Redshift', 'Trigger_Time', 'LAT_Boresight', 'Swift_Trigger_Number', 'GBM_Trigger_Number', 'Detection_Flags', 'Likelihood_Detection', 'LLE_Significance', 'Likelihood_Significance', 'Position_Source']\n",
      "\n",
      "📁 data\\raw\\Icecube_HESE.csv\n",
      "    Columns: ['id', 'mjd', 'ra_deg', 'dec_deg', 'f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'reconstruction', 'energy', 'drlogl']\n",
      "\n",
      "📁 data\\raw\\uzc_fk_compact.csv\n",
      "    Columns: ['ra_deg', 'dec_deg', 'Name', 'Con', 'Other', 'OT']\n",
      "\n",
      "📁 data\\registry\\basic_table_cleaned.csv\n",
      "    Columns: ['trigger', 'name', 'burst_id', 'tjd', 'time_sec', 'ra_deg', 'dec_deg', 'gal_lon', 'gal_lat', 'error_radius', 'geocenter_angle', 'overwrite_flag', 'overwritten_flag', 'jd', 'datetime']\n",
      "\n",
      "📁 data\\registry\\batse_master_grb_registry.csv\n",
      "    Columns: ['trigger', 'fluence_ch1', 'err_ch1', 'fluence_ch2', 'err_ch2', 'fluence_ch3', 'err_ch3', 'fluence_ch4', 'err_ch4', 'peak_flux_64ms', 'err_flux_64ms', 'time_flux_64ms', 'peak_flux_256ms', 'err_flux_256ms', 'time_flux_256ms', 'peak_flux_1024ms', 'err_flux_1024ms', 'time_flux_1024ms', 'name', 'burst_id', 'tjd', 'time_sec', 'ra_deg', 'dec_deg', 'gal_lon', 'gal_lat', 'error_radius', 'geocenter_angle', 'overwrite_flag', 'overwritten_flag', 'jd', 'datetime']\n",
      "\n",
      "📁 data\\registry\\grb_catalog_cleaned.csv\n",
      "    Columns: ['event_id', 'event_time', 'mjd', 'ra_deg', 'dec_deg', 'energy_GeV', 'reconstruction', 'drlogl']\n",
      "\n",
      "📁 data\\registry\\Open_Supernova_Catalog.csv\n",
      "    Columns: ['name', 'ra_deg', 'dec_deg', 'datetime', 'claimedtype', 'redshift']\n",
      "\n",
      "📁 data\\registry\\sne_1990s_cleaned.csv\n",
      "    Columns: ['name', 'ra_deg', 'dec_deg', 'redshift', 'datetime', 'maxabsmag', 'distance_mpc', 'type']\n",
      "\n",
      "📁 data\\registry\\data\\registry\\grb_catalogue_cleaned.csv\n",
      "    Columns: ['event_id', 'event_time', 'mjd', 'ra_deg', 'dec_deg', 'energy_GeV', 'reconstruction', 'drlogl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define root directory to start the search\n",
    "project_root = Path(\"..\").resolve()\n",
    "all_csv_files = list(project_root.rglob(\"*.csv\"))\n",
    "\n",
    "# Variants of coordinate column names we want to detect\n",
    "ra_variants = {\"ra\", \"RA\", \"ra_deg\", \"RA_deg\"}\n",
    "dec_variants = {\"dec\", \"DEC\", \"dec_deg\", \"DEC_deg\"}\n",
    "\n",
    "print(\"🔍 Scanning CSV files for RA/Dec variants...\\n\")\n",
    "\n",
    "# Scan each file\n",
    "for csv_file in all_csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file, nrows=5)  # Read only header and a few rows\n",
    "        cols = set(df.columns.str.lower())\n",
    "\n",
    "        has_ra = any(col in cols for col in {\"ra\", \"ra_deg\"})\n",
    "        has_dec = any(col in cols for col in {\"dec\", \"dec_deg\"})\n",
    "\n",
    "        if has_ra or has_dec:\n",
    "            print(f\"📁 {csv_file.relative_to(project_root)}\")\n",
    "            print(f\"    Columns: {list(df.columns)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped {csv_file.name} due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58f853cd-b8e1-4631-9284-9271e80f7329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Using registry: D:\\Bruno_Entropy_Project\\data\\registry\\bruno_entropy_event_log.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define project root dynamically (works inside /notebooks/)\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "# Core data directories\n",
    "registry_dir   = project_root / \"data\" / \"registry\"\n",
    "raw_data_dir   = project_root / \"data\" / \"raw\"\n",
    "snfluxes_dir   = raw_data_dir / \"snfluxes-public-master\"\n",
    "export_dir     = project_root / \"data\" / \"exports\"\n",
    "\n",
    "# Key data files\n",
    "\n",
    "fluence_table         = registry_dir / \"Supernova_Bruno_Fluence_Table.xlsx\"\n",
    "grb_path              = registry_dir / \"grb_catalog_cleaned.csv\"\n",
    "icecube_path          = raw_data_dir / \"Icecube_HESE.csv\"\n",
    "sn_flux_file          = snfluxes_dir / \"Nakazato_2013\" / \"nakazato-shen-z0.004-t_rev100ms-s20.0.fits\"\n",
    "uzc_fk_compact        = raw_data_dir / \"uzc_fk_compact.csv\"\n",
    "osc_path              = registry_dir / \"open_supernova_catalog.csv\"\n",
    "batse_path            = registry_dir / \"batse_master_grb_registry.csv\"\n",
    "crossmatch_path       = export_dir / \"sne_batse_crossmatches.csv\"\n",
    "sne_1990s_path        = registry_dir / \"sne_1990s_cleaned.csv\"\n",
    "\n",
    "# Define which registry you want to use\n",
    "selected_registry = \"bruno_entropy_event_log\"  # or \"bruno_events_registry\", \"data_file_registry\"\n",
    "\n",
    "# Safe filename mapping\n",
    "registry_files = {\n",
    "    \"bruno_events_registry\": \"bruno_events_registry.csv\",\n",
    "    \"bruno_entropy_event_log\": \"bruno_entropy_event_log.csv\",\n",
    "    \"data_file_registry\": \"data_file_registry.csv\"\n",
    "}\n",
    "\n",
    "# Resolve the full path\n",
    "registry_path = registry_dir / registry_files[selected_registry]\n",
    "\n",
    "# Confirm\n",
    "print(f\"📄 Using registry: {registry_path}\")\n",
    "\n",
    "\n",
    "\n",
    "# Load registry (example)\n",
    "registry = pd.read_csv(registry_path)\n",
    "\n",
    "# Load BATSE and compute fluence\n",
    "batse = pd.read_csv(batse_path)\n",
    "batse[\"fluence_total_erg_cm2\"] = (\n",
    "    batse[\"fluence_ch1\"].fillna(0) +\n",
    "    batse[\"fluence_ch2\"].fillna(0) +\n",
    "    batse[\"fluence_ch3\"].fillna(0) +\n",
    "    batse[\"fluence_ch4\"].fillna(0)\n",
    ")\n",
    "batse[\"fluence_total_J_m2\"] = batse[\"fluence_total_erg_cm2\"] * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d5dfa3e-3e7b-49c3-b6ed-1983bb743e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error in grb_icecube_crossmatch.csv: No columns to parse from file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Dynamically resolve path regardless of OS or cwd\n",
    "project_root = os.path.abspath(\"..\")  # Goes one level up from /notebooks/\n",
    "exports_path = os.path.join(project_root, \"data\", \"exports\")  # ✅ Correct\n",
    "\n",
    "# Fields to standardize\n",
    "rename_map = {\n",
    "    \"Fluence\": \"Fluence_at_Earth_J_per_m2\",\n",
    "    \"Fluence_J_m2\": \"Fluence_at_Earth_J_per_m2\",\n",
    "    \"RA (J2000)\": \"RA_J2000\",\n",
    "    \"Dec (J2000)\": \"Dec_J2000\",\n",
    "    \"Distance (Mpc)\": \"Distance_Mpc\",\n",
    "    \"Explosion Energy (erg)\": \"Explosion_Energy_erg\",\n",
    "    \"Bruno Trigger Time (s)\": \"Bruno_Trigger_Time_s\",\n",
    "    \"Detection Date (UTC)\": \"Detection_Date_UTC\"\n",
    "}\n",
    "\n",
    "# Loop and clean headers\n",
    "for f in os.listdir(exports_path):\n",
    "    if f.endswith(\".csv\"):\n",
    "        full_path = os.path.join(exports_path, f)\n",
    "        try:\n",
    "            df = pd.read_csv(full_path)\n",
    "            original_cols = df.columns.tolist()\n",
    "            cleaned = df.rename(columns=rename_map)\n",
    "            if cleaned.columns.tolist() != original_cols:\n",
    "                print(f\"🛠️ Fixed headers in: {f}\")\n",
    "                cleaned.to_csv(full_path, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {f}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdbfa877-9c27-467a-a087-2f5d9206b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "registry_dir = Path(\"../data/registry\")\n",
    "schema_files = list(registry_dir.glob(\"*.yaml\"))\n",
    "\n",
    "for schema_path in schema_files:\n",
    "    with open(schema_path, \"r\") as f:\n",
    "        schema = yaml.safe_load(f)\n",
    "\n",
    "    # Skip non-dict schemas\n",
    "    if not isinstance(schema, dict):\n",
    "        continue\n",
    "\n",
    "    updated = False\n",
    "    for key in list(schema.keys()):\n",
    "        # Fix field names\n",
    "        if \"fluence\" in key.lower():\n",
    "            schema[\"fluence_j_m2\"] = {\n",
    "                \"title\": \"Fluence at Earth\",\n",
    "                \"description\": \"Total gamma-ray burst fluence received at Earth in joules per square meter (J/m²)\",\n",
    "                \"unit\": \"J/m²\",\n",
    "                \"type\": \"number\"\n",
    "            }\n",
    "            del schema[key]\n",
    "            updated = True\n",
    "\n",
    "    # Save if updated\n",
    "    if updated:\n",
    "        with open(schema_path, \"w\") as f:\n",
    "            yaml.dump(schema, f)\n",
    "        print(f\"✅ Standardized fluence in: {schema_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b06fcbb-6faf-43c4-a107-c8c84121a6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Updated fluence column in: bruno_entropy_event_log.csv\n",
      "📝 Updated fluence column in: bruno_events_registry.csv\n",
      "📝 Updated fluence column in: Supernova_Bruno_Fluence_Table.csv\n"
     ]
    }
   ],
   "source": [
    "for file in registry_dir.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(file)\n",
    "    renamed = False\n",
    "\n",
    "    for col in df.columns:\n",
    "        if \"fluence\" in col.lower() and \"j\" in col.lower():\n",
    "            df.rename(columns={col: \"fluence_j_m2\"}, inplace=True)\n",
    "            renamed = True\n",
    "\n",
    "    if renamed:\n",
    "        df.to_csv(file, index=False)\n",
    "        print(f\"📝 Updated fluence column in: {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbad5b27-ad1d-4e93-9131-72c0ef9124ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "registry_dir = Path(\"../data/registry\")\n",
    "schema_files = list(registry_dir.glob(\"*.yaml\"))\n",
    "\n",
    "for schema_path in schema_files:\n",
    "    with open(schema_path, \"r\") as f:\n",
    "        schema = yaml.safe_load(f)\n",
    "\n",
    "    # Skip non-dict schemas\n",
    "    if not isinstance(schema, dict):\n",
    "        continue\n",
    "\n",
    "    updated = False\n",
    "    for key in list(schema.keys()):\n",
    "        # Fix field names\n",
    "        if \"fluence\" in key.lower():\n",
    "            schema[\"fluence_j_m2\"] = {\n",
    "                \"title\": \"Fluence at Earth\",\n",
    "                \"description\": \"Total gamma-ray burst fluence received at Earth in joules per square meter (J/m²)\",\n",
    "                \"unit\": \"J/m²\",\n",
    "                \"type\": \"number\"\n",
    "            }\n",
    "            del schema[key]\n",
    "            updated = True\n",
    "\n",
    "    # Save if updated\n",
    "    if updated:\n",
    "        with open(schema_path, \"w\") as f:\n",
    "            yaml.dump(schema, f)\n",
    "        print(f\"✅ Standardized fluence in: {schema_path.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61531dfc-e141-4c1d-8cc2-4a66d87124ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Updated fluence column in: bruno_entropy_event_log.csv\n",
      "📝 Updated fluence column in: bruno_events_registry.csv\n",
      "📝 Updated fluence column in: Supernova_Bruno_Fluence_Table.csv\n"
     ]
    }
   ],
   "source": [
    "for file in registry_dir.glob(\"*.csv\"):\n",
    "    df = pd.read_csv(file)\n",
    "    renamed = False\n",
    "\n",
    "    for col in df.columns:\n",
    "        if \"fluence\" in col.lower() and \"j\" in col.lower():\n",
    "            df.rename(columns={col: \"fluence_j_m2\"}, inplace=True)\n",
    "            renamed = True\n",
    "\n",
    "    if renamed:\n",
    "        df.to_csv(file, index=False)\n",
    "        print(f\"📝 Updated fluence column in: {file.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bea9a424-583d-40b8-ab95-9eb6ad28adec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Directory with schemas\n",
    "registry_dir = Path(\"../data/registry\")\n",
    "schema_files = list(registry_dir.glob(\"*.yaml\"))\n",
    "\n",
    "# Define the normalized fluence field\n",
    "normalized_key = \"fluence_j_m2\"\n",
    "standard_field = {\n",
    "    \"title\": \"Fluence at Earth\",\n",
    "    \"description\": \"Total gamma-ray burst fluence received at Earth in joules per square meter (J/m²)\",\n",
    "    \"unit\": \"J/m²\",\n",
    "    \"type\": \"number\"\n",
    "}\n",
    "\n",
    "def replace_fluence_fields(schema: dict) -> bool:\n",
    "    updated = False\n",
    "    if not isinstance(schema, dict):\n",
    "        return False\n",
    "\n",
    "    # Search top-level\n",
    "    for key in list(schema.keys()):\n",
    "        if \"fluence\" in key.lower() and key != normalized_key:\n",
    "            schema[normalized_key] = standard_field\n",
    "            del schema[key]\n",
    "            updated = True\n",
    "\n",
    "    # Search inside \"properties\"\n",
    "    if \"properties\" in schema and isinstance(schema[\"properties\"], dict):\n",
    "        props = schema[\"properties\"]\n",
    "        for key in list(props.keys()):\n",
    "            if \"fluence\" in key.lower() and key != normalized_key:\n",
    "                props[normalized_key] = standard_field\n",
    "                del props[key]\n",
    "                updated = True\n",
    "\n",
    "    return updated\n",
    "\n",
    "# Scan and apply changes\n",
    "for schema_path in schema_files:\n",
    "    try:\n",
    "        with open(schema_path, \"r\") as f:\n",
    "            schema = yaml.safe_load(f)\n",
    "\n",
    "        if replace_fluence_fields(schema):\n",
    "            with open(schema_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                yaml.dump(schema, f, sort_keys=False, allow_unicode=True)\n",
    "            print(f\"✅ Fixed fluence field in: {schema_path.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error reading {schema_path.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab9dcfe3-66e6-4fc5-bd08-4ba651e8b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Directory to scan for .csv files\n",
    "registry_dir = Path(\"../data/registry\")  # Adjust as needed\n",
    "csv_files = list(registry_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Known fluence column variants\n",
    "fluence_aliases = [\n",
    "    \"Fluence at Earth (J/m²)\",\n",
    "    \"fluence_total_J_m2\",\n",
    "    \"fluence_total_j_m2\",\n",
    "    \"fluence_total_erg_cm2\",  # Will convert if needed\n",
    "    \"Fluence at Earth\",\n",
    "]\n",
    "\n",
    "# Conversion factor from erg/cm² to J/m²\n",
    "erg_to_joules = 0.1\n",
    "\n",
    "def fix_fluence_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for alias in fluence_aliases:\n",
    "        if alias in df.columns:\n",
    "            # If the column is in erg/cm², convert it\n",
    "            if alias == \"fluence_total_erg_cm2\":\n",
    "                df[\"fluence_j_m2\"] = df[alias] * erg_to_joules\n",
    "            else:\n",
    "                df[\"fluence_j_m2\"] = df[alias]\n",
    "            df.drop(columns=[alias], inplace=True)\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "# Process each CSV\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        backup_path = file.with_suffix(\".csv.bak\")\n",
    "        file.rename(backup_path)\n",
    "        df.to_csv(file, index=False)\n",
    "\n",
    "        original_cols = set(df.columns)\n",
    "        df = fix_fluence_column(df)\n",
    "        if \"fluence_j_m2\" in df.columns and set(df.columns) != original_cols:\n",
    "            df.to_csv(file, index=False)\n",
    "            print(f\"✅ Updated fluence field in: {file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not process {file.name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8b5f2d1-6877-4d05-95a1-4a17a0c75480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ No updates were made to any files.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Directory with .csv files\n",
    "registry_dir = Path(\"../data/registry\")  # Update if needed\n",
    "csv_files = list(registry_dir.glob(\"*.csv\"))\n",
    "\n",
    "# Known variants\n",
    "fluence_aliases = {\n",
    "    \"Fluence at Earth (J/m²)\": \"direct\",\n",
    "    \"fluence_total_J_m2\": \"direct\",\n",
    "    \"fluence_total_j_m2\": \"direct\",\n",
    "    \"Fluence at Earth\": \"direct\",\n",
    "    \"fluence_total_erg_cm2\": \"convert\",\n",
    "}\n",
    "\n",
    "# Conversion factor for erg/cm² to J/m²\n",
    "erg_to_joules = 0.1\n",
    "\n",
    "# Tracking log\n",
    "log = []\n",
    "\n",
    "def fix_fluence_column(df: pd.DataFrame, file_name: str) -> pd.DataFrame:\n",
    "    for col, mode in fluence_aliases.items():\n",
    "        if col in df.columns:\n",
    "            if mode == \"convert\":\n",
    "                df[\"fluence_j_m2\"] = df[col] * erg_to_joules\n",
    "            else:\n",
    "                df[\"fluence_j_m2\"] = df[col]\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "            log.append({\"file\": file_name, \"original_column\": col, \"action\": mode})\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "# Process files\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file)\n",
    "        cols_before = set(df.columns)\n",
    "        df = fix_fluence_column(df, file.name)\n",
    "        if \"fluence_j_m2\" in df.columns and set(df.columns) != cols_before:\n",
    "            df.to_csv(file, index=False)\n",
    "            print(f\"✅ Updated fluence field in: {file.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped {file.name}: {e}\")\n",
    "\n",
    "# Save report log\n",
    "if log:\n",
    "    report_df = pd.DataFrame(log)\n",
    "    report_path = registry_dir / \"fluency_column_update_report.csv\"\n",
    "    report_df.to_csv(report_path, index=False)\n",
    "    print(f\"📄 Report saved to: {report_path}\")\n",
    "else:\n",
    "    print(\"ℹ️ No updates were made to any files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "acb2c08f-b0cf-4036-8191-58285cf8e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_schema_from_csv(csv_path, yaml_path):\n",
    "    import pandas as pd, yaml\n",
    "\n",
    "    df = pd.read_csv(csv_path, nrows=5)  # sample first few rows\n",
    "    schema = {\n",
    "        \"columns\": [\n",
    "            {\n",
    "                \"name\": col,\n",
    "                \"type\": str(df[col].dtype),\n",
    "                \"description\": \"TBD\"\n",
    "            } for col in df.columns\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(schema, f, sort_keys=False)\n",
    "\n",
    "    print(f\"✅ Schema written to {yaml_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a71e86eb-b69a-422d-b167-db700e310859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Headers cleaned and original backed up to: ..\\data\\registry\\bruno_entropy_event_log.csv.bak\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "log_path = Path(\"../data/registry/bruno_entropy_event_log.csv\")\n",
    "backup_path = log_path.with_suffix(\".csv.bak\")\n",
    "\n",
    "rename_map = {\n",
    "    \"Event Name\": \"Event_Name\",\n",
    "    \"Detection Date (UTC)\": \"Detection_Date_UTC\",\n",
    "    \"Source Galaxy\": \"Source_Galaxy\",\n",
    "    \"RA (J2000)\": \"RA_J2000\",\n",
    "    \"Dec (J2000)\": \"Dec_J2000\",\n",
    "    \"Distance (Mpc)\": \"Distance_Mpc\",\n",
    "    \"Explosion Energy (erg)\": \"Explosion_Energy_erg\",\n",
    "    \"Bruno Trigger Time (s)\": \"Bruno_Trigger_Time_s\",\n",
    "    \"Estimated Collapse Time (UTC)\": \"Estimated_Collapse_Time_UTC\",\n",
    "    \"Fluence at Earth (J/m²)\": \"Fluence_at_Earth_J_per_m2\",\n",
    "    \"Bruno Threshold Crossed\": \"Bruno_Threshold_Crossed\",\n",
    "    \"Neutrino Detected\": \"Neutrino_Detected\",\n",
    "    \"Neutrino Energy (TeV)\": \"Neutrino_Energy_TeV\",\n",
    "    \"Positional Match Confidence\": \"Positional_Match_Confidence\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(log_path)\n",
    "df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "log_path.rename(backup_path)\n",
    "df.to_csv(log_path, index=False)\n",
    "print(f\"✅ Headers cleaned and original backed up to: {backup_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9eff131-f828-4dec-aaec-58719a15cf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- YAML PREVIEW ---\n",
      "\n",
      "dataset_name: Bruno_Highlight_Events\n",
      "description: TBD - describe this dataset\n",
      "columns:\n",
      "- name: Name\n",
      "  type: object\n",
      "  description: TBD\n",
      "- name: RA (deg)\n",
      "  type: float64\n",
      "  description: TBD\n",
      "- name: Dec (deg)\n",
      "  type: float64\n",
      "  description: TBD\n",
      "- name: Discovery Date\n",
      "  type: object\n",
      "  description: TBD\n",
      "- name: Distance_Mpc\n",
      "  type: float64\n",
      "  description: TBD\n",
      "- name: \"Fluence (J/m\\xB2)\"\n",
      "  type: float64\n",
      "  description: TBD\n",
      "- name: Label\n",
      "  type: object\n",
      "  description: TBD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_yaml_preview(csv_path, nrows=5):\n",
    "    csv_path = Path(csv_path)\n",
    "    df = pd.read_csv(csv_path, nrows=nrows)\n",
    "\n",
    "    schema = {\n",
    "        \"dataset_name\": csv_path.stem,\n",
    "        \"description\": \"TBD - describe this dataset\",\n",
    "        \"columns\": []\n",
    "    }\n",
    "\n",
    "    for col in df.columns:\n",
    "        dtype = str(df[col].dtype)\n",
    "        schema[\"columns\"].append({\n",
    "            \"name\": col,\n",
    "            \"type\": dtype,\n",
    "            \"description\": \"TBD\"\n",
    "        })\n",
    "\n",
    "    # Print to screen only\n",
    "    print(\"\\n--- YAML PREVIEW ---\\n\")\n",
    "    print(yaml.dump(schema, sort_keys=False, default_flow_style=False))\n",
    "\n",
    "    # Optional: Save if you want\n",
    "    # yaml_path = csv_path.with_suffix(\".schema.yaml\")\n",
    "    # with open(yaml_path, \"w\") as f:\n",
    "    #     yaml.dump(schema, f)\n",
    "    # print(f\"✅ Schema saved: {yaml_path}\")\n",
    "\n",
    "# EXAMPLE USE:\n",
    "generate_yaml_preview(\"../data/exports/final/Bruno_Highlight_Events.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eea6dfdf-9353-439f-af50-a17532fb2f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Find all CSV and YAML schema files\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[43mextract_path\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      6\u001b[0m yaml_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(extract_path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Build a map of dataset base names → schema path\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_path' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Find all CSV and YAML schema files\n",
    "csv_files = list(extract_path.glob(\"*.csv\"))\n",
    "yaml_files = list(extract_path.glob(\"*.yaml\"))\n",
    "\n",
    "# Build a map of dataset base names → schema path\n",
    "yaml_map = {f.stem.replace(\"_schema\", \"\"): f for f in yaml_files}\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "# Validate each CSV against its matching YAML schema\n",
    "for csv_path in csv_files:\n",
    "    base_name = csv_path.stem\n",
    "    if base_name in yaml_map:\n",
    "        yaml_path = yaml_map[base_name]\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, nrows=1)  # Just read headers\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                schema = yaml.safe_load(f)\n",
    "\n",
    "            csv_columns = set(df.columns)\n",
    "            schema_columns = {col['name'] for col in schema.get(\"columns\", [])}\n",
    "\n",
    "            missing_in_csv = schema_columns - csv_columns\n",
    "            extra_in_csv = csv_columns - schema_columns\n",
    "\n",
    "            validation_results.append({\n",
    "                \"csv\": csv_path.name,\n",
    "                \"schema\": yaml_path.name,\n",
    "                \"missing_in_csv\": list(missing_in_csv),\n",
    "                \"extra_in_csv\": list(extra_in_csv),\n",
    "                \"status\": \"✅ PASS\" if not missing_in_csv and not extra_in_csv else \"⚠️ MISMATCH\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            validation_results.append({\n",
    "                \"csv\": csv_path.name,\n",
    "                \"schema\": yaml_path.name,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \"❌ ERROR\"\n",
    "            })\n",
    "    else:\n",
    "        validation_results.append({\n",
    "            \"csv\": csv_path.name,\n",
    "            \"schema\": None,\n",
    "            \"status\": \"❗ NO SCHEMA FOUND\"\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"CSV Schema Validation\", dataframe=pd.DataFrame(validation_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912457b3-7a26-432a-a1ac-bc8482bf8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Redefine extract_path since it's out of scope in this new cell\n",
    "extract_path = Path(\"/mnt/data/registry_unzipped\")\n",
    "\n",
    "# Re-run the CSV/YAML validation logic\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Find all CSV and YAML schema files\n",
    "csv_files = list(extract_path.glob(\"*.csv\"))\n",
    "yaml_files = list(extract_path.glob(\"*.yaml\"))\n",
    "\n",
    "# Build a map of dataset base names → schema path\n",
    "yaml_map = {f.stem.replace(\"_schema\", \"\"): f for f in yaml_files}\n",
    "\n",
    "validation_results = []\n",
    "\n",
    "# Validate each CSV against its matching YAML schema\n",
    "for csv_path in csv_files:\n",
    "    base_name = csv_path.stem\n",
    "    if base_name in yaml_map:\n",
    "        yaml_path = yaml_map[base_name]\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path, nrows=1)  # Just read headers\n",
    "            with open(yaml_path, 'r') as f:\n",
    "                schema = yaml.safe_load(f)\n",
    "\n",
    "            csv_columns = set(df.columns)\n",
    "            schema_columns = {col['name'] for col in schema.get(\"columns\", [])}\n",
    "\n",
    "            missing_in_csv = schema_columns - csv_columns\n",
    "            extra_in_csv = csv_columns - schema_columns\n",
    "\n",
    "            validation_results.append({\n",
    "                \"csv\": csv_path.name,\n",
    "                \"schema\": yaml_path.name,\n",
    "                \"missing_in_csv\": list(missing_in_csv),\n",
    "                \"extra_in_csv\": list(extra_in_csv),\n",
    "                \"status\": \"✅ PASS\" if not missing_in_csv and not extra_in_csv else \"⚠️ MISMATCH\"\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            validation_results.append({\n",
    "                \"csv\": csv_path.name,\n",
    "                \"schema\": yaml_path.name,\n",
    "                \"error\": str(e),\n",
    "                \"status\": \" ERROR\"\n",
    "            })\n",
    "    else:\n",
    "        validation_results.append({\n",
    "            \"csv\": csv_path.name,\n",
    "            \"schema\": None,\n",
    "            \"status\": \" NO SCHEMA FOUND\"\n",
    "        })\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"CSV Schema Validation\", dataframe=pd.DataFrame(validation_results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b33924-e9de-4f49-a4a8-0597fda13719",
   "metadata": {},
   "source": [
    "# 🧠 Bruno Entropy Project — Structure Notebook\n",
    "\n",
    "This notebook provides the **core structure and starter paths** for accessing all datasets and outputs within the `Bruno_Entropy_Project`.\n",
    "\n",
    "Use this as your consistent baseline for:\n",
    "\n",
    "- 🔍 Loading and inspecting cleaned registries\n",
    "- 🔄 Accessing curated outputs and crossmatches\n",
    "- 📦 Linking entropy fluence models with matched SN–GRB–IceCube events\n",
    "- 📚 Ensuring schema-validated data science workflows\n",
    "\n",
    "---\n",
    "\n",
    "## 📁 Directory Layout Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c95387-241f-47ae-b4ae-ae797b6f3e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Using registry: D:\\Bruno_Entropy_Project\\data\\registry\\bruno_entropy_event_log.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Dynamically define project root (assuming this notebook is in /notebooks/)\n",
    "project_root = Path(\"..\").resolve()\n",
    "\n",
    "# 📁 Core data directories\n",
    "registry_dir   = project_root / \"data\" / \"registry\"\n",
    "raw_data_dir   = project_root / \"data\" / \"raw\"\n",
    "export_dir     = project_root / \"data\" / \"exports\"\n",
    "snfluxes_dir   = raw_data_dir / \"snfluxes-public-master\"\n",
    "\n",
    "# 📁 Subdirectories inside exports\n",
    "final_dir         = export_dir / \"final\"\n",
    "crossmatch_dir    = export_dir / \"crossmatch\"\n",
    "entropy_tables_dir = export_dir / \"entropy_tables\"\n",
    "\n",
    "# 📄 Registry-level data files\n",
    "fluence_table         = registry_dir / \"Supernova_Bruno_Fluence_Table.csv\"\n",
    "grb_path              = registry_dir / \"grb_catalog_cleaned.csv\"\n",
    "icecube_path          = raw_data_dir / \"Icecube_HESE.csv\"\n",
    "sn_flux_file          = snfluxes_dir / \"Nakazato_2013\" / \"nakazato-shen-z0.004-t_rev100ms-s20.0.fits\"\n",
    "uzc_fk_compact        = raw_data_dir / \"uzc_fk_compact.csv\"\n",
    "osc_path              = registry_dir / \"open_supernova_catalog.csv\"\n",
    "batse_path            = registry_dir / \"batse_master_grb_registry.csv\"\n",
    "sne_1990s_path        = registry_dir / \"sne_1990s_cleaned.csv\"\n",
    "icecat1_path          = raw_data_dir / \"icecat1.csv\"\n",
    "\n",
    "\n",
    "# 🧠 Registry selector\n",
    "selected_registry = \"bruno_entropy_event_log\"  # options: bruno_events_registry, data_file_registry\n",
    "registry_files = {\n",
    "    \"bruno_events_registry\": \"bruno_events_registry.csv\",\n",
    "    \"bruno_entropy_event_log\": \"bruno_entropy_event_log.csv\",\n",
    "    \"data_file_registry\": \"data_file_registry.csv\"\n",
    "}\n",
    "registry_path = registry_dir / registry_files[selected_registry]\n",
    "print(f\"📄 Using registry: {registry_path}\")\n",
    "\n",
    "# 📄 Final curated datasets\n",
    "highlight_path       = final_dir / \"Bruno_Highlight_Events.csv\"\n",
    "clustered_candidates = final_dir / \"bruno_clustered_entropy_candidates.csv\"\n",
    "relaxed_candidates   = final_dir / \"bruno_relaxed_entropy_candidates.csv\"\n",
    "typed_relaxed        = final_dir / \"typed_bruno_relaxed_candidates.csv\"\n",
    "sn_bruno_path        = final_dir / \"sn_bruno_candidates.csv\"\n",
    "\n",
    "# 📄 Crossmatch datasets\n",
    "batse_sn_crossmatch  = crossmatch_dir / \"sne_batse_crossmatches.csv\"\n",
    "grb_icecube_cross    = crossmatch_dir / \"grb_icecube_crossmatch.csv\"\n",
    "bruno_icecube_cross  = crossmatch_dir / \"bruno_icecube_crossmatch.csv\"\n",
    "sn_grb_matrix_path   = crossmatch_dir / \"bruno_sn_grb_matrix.csv\"\n",
    "\n",
    "# 📄 Entropy output tables\n",
    "entropy_data_path         = entropy_tables_dir / \"Entropy_Data.csv\"\n",
    "entropy_horizon_path      = entropy_tables_dir / \"Entropy_Horizon_Table.csv\"\n",
    "entropy_water_vapor_path  = entropy_tables_dir / \"Entropy_Water_Vapor.csv\"\n",
    "\n",
    "# ✅ Example usage\n",
    "registry = pd.read_csv(registry_path)\n",
    "batse = pd.read_csv(batse_path)\n",
    "\n",
    "# Compute BATSE total fluence\n",
    "batse[\"fluence_total_erg_cm2\"] = (\n",
    "    batse[\"fluence_ch1\"].fillna(0) +\n",
    "    batse[\"fluence_ch2\"].fillna(0) +\n",
    "    batse[\"fluence_ch3\"].fillna(0) +\n",
    "    batse[\"fluence_ch4\"].fillna(0)\n",
    ")\n",
    "batse[\"fluence_total_J_m2\"] = batse[\"fluence_total_erg_cm2\"] * 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e88ecac9-8eab-495b-aa29-22bd82a84e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Registry preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event_Name</th>\n",
       "      <th>Detection_Date_UTC</th>\n",
       "      <th>Source_Galaxy</th>\n",
       "      <th>RA_J2000</th>\n",
       "      <th>Dec_J2000</th>\n",
       "      <th>Distance_Mpc</th>\n",
       "      <th>Explosion_Energy_erg</th>\n",
       "      <th>Bruno_Trigger_Time_s</th>\n",
       "      <th>Estimated_Collapse_Time_UTC</th>\n",
       "      <th>fluence_j_m2</th>\n",
       "      <th>Bruno_Threshold_Crossed</th>\n",
       "      <th>Neutrino_Detected</th>\n",
       "      <th>Neutrino_Energy_TeV</th>\n",
       "      <th>Positional_Match_Confidence</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IceCube-49427574</td>\n",
       "      <td>2025-03-30 08:31:06</td>\n",
       "      <td>UGC 11572</td>\n",
       "      <td>307.29</td>\n",
       "      <td>10.74</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1.000000e+52</td>\n",
       "      <td>1.936</td>\n",
       "      <td>2025-03-30 08:31:04.064</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>147.57</td>\n",
       "      <td>High</td>\n",
       "      <td>Confirmed spatial + temporal match. First offi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Event_Name   Detection_Date_UTC Source_Galaxy  RA_J2000  Dec_J2000  \\\n",
       "0  IceCube-49427574  2025-03-30 08:31:06     UGC 11572    307.29      10.74   \n",
       "\n",
       "   Distance_Mpc  Explosion_Energy_erg  Bruno_Trigger_Time_s  \\\n",
       "0          63.0          1.000000e+52                 1.936   \n",
       "\n",
       "  Estimated_Collapse_Time_UTC  fluence_j_m2  Bruno_Threshold_Crossed  \\\n",
       "0     2025-03-30 08:31:04.064      0.000021                     True   \n",
       "\n",
       "   Neutrino_Detected  Neutrino_Energy_TeV Positional_Match_Confidence  \\\n",
       "0               True               147.57                        High   \n",
       "\n",
       "                                               Notes  \n",
       "0  Confirmed spatial + temporal match. First offi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"📊 Registry preview:\")\n",
    "display(registry.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b515a1f4-a0d6-4de7-9309-16108648c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema loaded with 15 columns\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "schema_path = registry_path.with_name(registry_path.stem + \"_schema.yaml\")\n",
    "if schema_path.exists():\n",
    "    with open(schema_path) as f:\n",
    "        schema = yaml.safe_load(f)\n",
    "    print(\"✅ Schema loaded with\", len(schema['columns']), \"columns\")\n",
    "else:\n",
    "    print(\"⚠️ No schema found:\", schema_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d057f7a8-4668-491c-bb94-1c65ae9e5498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Available registries: ['basic_table_cleaned', 'BATSE_Flux_Catalog', 'batse_master_grb_registry', 'bruno_entropy_event_log', 'bruno_events_registry', 'data_file_registry', 'grb_catalog_cleaned', 'Open_Supernova_Catalog', 'registry', 'sne_1990s_cleaned', 'Supernova_Bruno_Fluence_Table']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "available_registries = [Path(f).stem for f in glob(str(registry_dir / \"*.csv\"))]\n",
    "print(\"📁 Available registries:\", available_registries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf888ec6-1f8f-4cd1-920e-ba8308c6e87e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Bruno_Entropy_Project\\\\data\\\\raw\\\\icecat1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load IceCat-1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m icecat_path \u001b[38;5;241m=\u001b[39m raw_data_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124micecat1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m icecat \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43micecat_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Convert MJD to datetime\u001b[39;00m\n\u001b[0;32m      9\u001b[0m icecat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Time(icecat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEVENTMJD\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmjd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_datetime()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Bruno_Entropy_Project\\\\data\\\\raw\\\\icecat1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from astropy.time import Time\n",
    "\n",
    "# Load IceCat-1\n",
    "icecat_path = raw_data_dir / \"icecat1.csv\"\n",
    "icecat = pd.read_csv(icecat_path)\n",
    "\n",
    "# Convert MJD to datetime\n",
    "icecat[\"event_time\"] = Time(icecat[\"EVENTMJD\"], format=\"mjd\").to_datetime()\n",
    "\n",
    "# Standardize position columns\n",
    "icecat[\"ra_deg\"] = icecat[\"RA\"]\n",
    "icecat[\"dec_deg\"] = icecat[\"DEC\"]\n",
    "\n",
    "# Optional: Event filter by quality or energy\n",
    "icecat = icecat[icecat[\"ENERGY\"] > 100]  # TeV-scale neutrinos\n",
    "\n",
    "# Preview\n",
    "icecat[[\"event_time\", \"ra_deg\", \"dec_deg\", \"ENERGY\", \"I3TYPE\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f422bd-fadd-4a44-a952-aa632940cdae",
   "metadata": {},
   "source": [
    "### Kato data extract ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c0ad07d-6d63-47da-951b-a35e3bb7bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0ce4722-cbb7-4aba-98e5-f89bc57017d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Bruno_Entropy_Project/data/raw/Kato_2017/collapse/m15/total_nux/lightcurve.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ✅ Step 1: Load Kato m15 ν_x luminosity data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:/Bruno_Entropy_Project/data/raw/Kato_2017/collapse/m15/total_nux/lightcurve.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df_kato \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Bruno_Entropy_Project/data/raw/Kato_2017/collapse/m15/total_nux/lightcurve.dat'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ✅ Step 1: Load Kato m15 ν_x luminosity data\n",
    "file_path = \"D:/Bruno_Entropy_Project/data/raw/Kato_2017/collapse/m15/total_nux/lightcurve.dat\"\n",
    "df_kato = pd.read_csv(file_path, sep=r\"\\s+\", header=None, comment=\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec1d1806-c25e-4d3a-88db-d33e3ae3d230",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lum_kato' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mev_to_erg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.602e-6\u001b[39m\n\u001b[1;32m----> 2\u001b[0m lum_kato_erg \u001b[38;5;241m=\u001b[39m \u001b[43mlum_kato\u001b[49m \u001b[38;5;241m*\u001b[39m mev_to_erg\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lum_kato' is not defined"
     ]
    }
   ],
   "source": [
    "# ✅ Step 2: Extract relevant columns\n",
    "time_kato = df_kato[1]  # Time after bounce [s]\n",
    "lum_mev_s = df_kato[7]  # Energy luminosity [MeV/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f99dd761-6ce3-4e95-93de-12304f1e4166",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_kato' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m distance_kpc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m      4\u001b[0m distance_cm \u001b[38;5;241m=\u001b[39m distance_kpc \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3.086e21\u001b[39m\n\u001b[1;32m----> 6\u001b[0m dt \u001b[38;5;241m=\u001b[39m \u001b[43mtime_kato\u001b[49m\u001b[38;5;241m.\u001b[39mdiff()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m fluence_kato \u001b[38;5;241m=\u001b[39m (lum_kato_erg \u001b[38;5;241m*\u001b[39m dt)\u001b[38;5;241m.\u001b[39mcumsum() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi \u001b[38;5;241m*\u001b[39m distance_cm\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_kato' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "distance_kpc = 50\n",
    "distance_cm = distance_kpc * 3.086e21\n",
    "\n",
    "dt = time_kato.diff().fillna(0)\n",
    "fluence_kato = (lum_kato_erg * dt).cumsum() / (4 * np.pi * distance_cm**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2137bc67-68f6-438d-8409-778be8cf923a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fluence_kato' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m avg_energy_mev \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m----> 2\u001b[0m fluence_norm_kato \u001b[38;5;241m=\u001b[39m \u001b[43mfluence_kato\u001b[49m \u001b[38;5;241m/\u001b[39m (avg_energy_mev \u001b[38;5;241m*\u001b[39m mev_to_erg)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fluence_kato' is not defined"
     ]
    }
   ],
   "source": [
    "avg_energy_mev = 15\n",
    "fluence_norm_kato = fluence_kato / (avg_energy_mev * mev_to_erg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2db99eda-6363-4595-8038-ef1825944f2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(time_kato, fluence_norm_kato, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKato m15 ν_x\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab:purple\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(time_15, fluence_norm_15, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBruno 15 Msun\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtab:blue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_kato, fluence_norm_kato, label=\"Kato m15 ν_x\", color=\"tab:purple\")\n",
    "plt.plot(time_15, fluence_norm_15, label=\"Bruno 15 Msun\", color=\"tab:blue\")\n",
    "plt.plot(time_nakazato, fluence_norm_nakazato, label=\"Nakazato\", color=\"tab:green\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Neutrino Count (particles/cm²)\")\n",
    "plt.title(\"Kato vs Bruno vs Nakazato — Particle Fluence\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87a7bb3e-6dd0-493b-a161-94bd1caa65eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned SKII_SKI_table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tan2_theta</th>\n",
       "      <th>delta_m2</th>\n",
       "      <th>chi2</th>\n",
       "      <th>B8</th>\n",
       "      <th>hep</th>\n",
       "      <th>B8_unc</th>\n",
       "      <th>hep_unc</th>\n",
       "      <th>cross_section</th>\n",
       "      <th>B8_norm</th>\n",
       "      <th>es_I</th>\n",
       "      <th>er_I</th>\n",
       "      <th>es_II</th>\n",
       "      <th>er_II</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>24.5585</td>\n",
       "      <td>2.3156</td>\n",
       "      <td>21.3428</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>17.7367</td>\n",
       "      <td>1.3177</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.3346</td>\n",
       "      <td>-0.1233</td>\n",
       "      <td>-0.3047</td>\n",
       "      <td>-0.2724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.047000e-09</td>\n",
       "      <td>24.5585</td>\n",
       "      <td>2.3156</td>\n",
       "      <td>21.3511</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>17.7377</td>\n",
       "      <td>1.3177</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.3052</td>\n",
       "      <td>-0.2716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.096000e-09</td>\n",
       "      <td>24.5586</td>\n",
       "      <td>2.3156</td>\n",
       "      <td>21.3522</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>17.7378</td>\n",
       "      <td>1.3177</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>-0.1226</td>\n",
       "      <td>-0.3053</td>\n",
       "      <td>-0.2718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.148000e-09</td>\n",
       "      <td>24.5587</td>\n",
       "      <td>2.3156</td>\n",
       "      <td>21.3522</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>17.7378</td>\n",
       "      <td>1.3177</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>-0.1226</td>\n",
       "      <td>-0.3053</td>\n",
       "      <td>-0.2718</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tan2_theta      delta_m2     chi2      B8      hep  B8_unc  hep_unc  \\\n",
       "0         NaN           NaN      NaN     NaN      NaN     NaN      NaN   \n",
       "1      0.0001  1.000000e-09  24.5585  2.3156  21.3428  0.0652  17.7367   \n",
       "2      0.0001  1.047000e-09  24.5585  2.3156  21.3511  0.0652  17.7377   \n",
       "3      0.0001  1.096000e-09  24.5586  2.3156  21.3522  0.0652  17.7378   \n",
       "4      0.0001  1.148000e-09  24.5587  2.3156  21.3522  0.0652  17.7378   \n",
       "\n",
       "   cross_section  B8_norm    es_I    er_I   es_II   er_II  \n",
       "0            NaN      NaN     NaN     NaN     NaN     NaN  \n",
       "1         1.3177   0.0924  0.3346 -0.1233 -0.3047 -0.2724  \n",
       "2         1.3177   0.0925  0.3349 -0.1224 -0.3052 -0.2716  \n",
       "3         1.3177   0.0927  0.3349 -0.1226 -0.3053 -0.2718  \n",
       "4         1.3177   0.0927  0.3349 -0.1226 -0.3053 -0.2718  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Load SKII_SKI_table.dat safely\n",
    "ski_table_path = \"D:/Bruno_Entropy_Project/data/raw/SK_Baksan/SKII_SKI_table.dat\"\n",
    "\n",
    "# Step 1: Load everything as string to avoid dtype warnings\n",
    "df_ski = pd.read_csv(ski_table_path, sep=r\"\\s+\", header=None, dtype=str)\n",
    "\n",
    "# Step 2: Assign column names (13 columns expected)\n",
    "df_ski.columns = [\n",
    "    \"tan2_theta\", \"delta_m2\", \"chi2\", \"B8\", \"hep\",\n",
    "    \"B8_unc\", \"hep_unc\", \"cross_section\",\n",
    "    \"B8_norm\", \"es_I\", \"er_I\", \"es_II\", \"er_II\"\n",
    "]\n",
    "\n",
    "# Step 3: Replace '--' or other placeholders with NaN and convert to numeric\n",
    "df_ski.replace({\"--\": np.nan, \"*\": np.nan}, inplace=True)\n",
    "for col in df_ski.columns:\n",
    "    df_ski[col] = pd.to_numeric(df_ski[col], errors=\"coerce\")\n",
    "\n",
    "# ✅ Preview result\n",
    "print(\"✅ Cleaned SKII_SKI_table:\")\n",
    "display(df_ski.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f2be8-5188-485c-a8df-6d59aafe0501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
